{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3127856-c9c9-408f-beaf-f4f3edf068a8",
   "metadata": {},
   "source": [
    "# 目标概要\n",
    "\n",
    "- 指导如何部署neural-sparse模型(OpenSearch内置模型)和neural-dense模型(bedrock:Cohere Embedding)\n",
    "- 指导如何在ingestion 和 query时使用这些模型\n",
    "- 指导如何实现两路的Hybird查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a277c8-7f27-4f36-89fa-244a65fef8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install requests_aws4auth\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416b5ac-9bc4-4c84-a950-5311ffb27d18",
   "metadata": {},
   "source": [
    "## 0. 前置步骤\n",
    "- 进入OpenSearch Dashboard 的 dev tool <br>\n",
    "  对于部署在VPC中的OpenSearch cluster，需要通过该VPC的ec2进行访问中转，具体可以参考workshop其中一[小节](https://catalog.us-east-1.prod.workshops.aws/workshops/158a2497-7cbe-4ba4-8bee-2307cb01c08a/en-US/4-runpipeline/setupknowledgebase) <br> <br>\n",
    "- 注册一个model_group\n",
    "    ```http\n",
    "    POST /_plugins/_ml/model_groups/_register\n",
    "    {\n",
    "      \"name\": \"remote_model_group\",\n",
    "      \"description\": \"A model group for remote models\"\n",
    "    }\n",
    "    ```\n",
    "    <br>\n",
    "    输出:\n",
    "    \n",
    "    ```json\n",
    "    {\n",
    "      \"model_group_id\": \"zksL94sB0S9ucTLoj1u0\",\n",
    "      \"status\": \"CREATED\"\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a5afe-9c50-47cb-88a3-c0ca33b90b96",
   "metadata": {},
   "source": [
    "## 1. 部署neural-sparse模型 \n",
    "\n",
    "### 2.11版本(部署在SageMaker中)\n",
    "\n",
    "- 进入OpenSearch的Integration页面\n",
    "\n",
    "  ![integration_1.png](./integration_1.png)\n",
    "  按照cloudformation template的要求填完以后，会有模型部署在sagemaker，且已经在OpenSearch中注册好model\n",
    "\n",
    "- 验证部署的nerual-sparse模型\n",
    "  + 进入cloudformation对应stack，切换到output, 获取modelId, ConnecterId 以及Sagemaker endpoint\n",
    "    ![nerual-sparse.png](./nerual-sparse.png)\n",
    "  + dev tool中执行如下脚本\n",
    "    ```http\n",
    "    GET /_plugins/_ml/models/<model_id>\n",
    "    ```\n",
    "    <br>\n",
    "    输出\n",
    "    \n",
    "    ```json\n",
    "    {\n",
    "      \"name\": \"sagemaker-model-for-connector-1UsW_4sB0S9ucTLoyVsG\",\n",
    "      \"model_group_id\": \"1ksW_4sB0S9ucTLoyVul\",\n",
    "      \"algorithm\": \"REMOTE\",\n",
    "      \"model_version\": \"1\",\n",
    "      \"description\": \"Sagemaker Model for connector 1UsW_4sB0S9ucTLoyVsG\",\n",
    "      \"model_state\": \"DEPLOYED\",\n",
    "      \"created_time\": 1700791765463,\n",
    "      \"last_updated_time\": 1700791765549,\n",
    "      \"last_deployed_time\": 1700791765549,\n",
    "      \"planning_worker_node_count\": 2,\n",
    "      \"current_worker_node_count\": 2,\n",
    "      \"planning_worker_nodes\": [\n",
    "        \"W5BqyJqbRr2GPVkjwgoaqQ\",\n",
    "        \"dbjOCw5sSBuIKgKz3CKXjQ\"\n",
    "      ],\n",
    "      \"deploy_to_all_nodes\": true,\n",
    "      \"connector_id\": \"1UsW_4sB0S9ucTLoyVsG\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "- 测试部署的SPLADE模型\n",
    "  ```http\n",
    "  POST /_plugins/_ml/models/2EsW_4sB0S9ucTLoyVvY/_predict\n",
    "  {\n",
    "    \"parameters\": {\n",
    "      \"inputs\": \"Hi Altman\"\n",
    "    }\n",
    "  }\n",
    "  ```\n",
    "  <br>\n",
    "  输出\n",
    "  <br>\n",
    "  \n",
    "  ```json\n",
    "  {\n",
    "    \"inference_results\": [\n",
    "      {\n",
    "        \"output\": [\n",
    "          {\n",
    "            \"name\": \"response\",\n",
    "            \"dataAsMap\": {\n",
    "              \"response\": [\n",
    "                {\n",
    "                  \"e\": 0.1419215202331543,\n",
    "                  \"he\": 0.33063653111457825,\n",
    "                  \"his\": 0.424188494682312,\n",
    "                  \"she\": 0.10910777002573013,\n",
    "                  \"him\": 0.05982781946659088,\n",
    "                  \"who\": 0.47575441002845764,\n",
    "                  \"american\": 0.011252160184085369,\n",
    "                  ...\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"status_code\": 200\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d257786-ac00-4bd6-9c69-9a55bde2474e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.12版本（直接部署在OpenSearch集群中，尚未发布）\n",
    "\n",
    "- 设置权限\n",
    "  ```http\n",
    "    PUT /_cluster/settings\n",
    "    {\n",
    "        \"persistent\": {\n",
    "            \"plugins.ml_commons.only_run_on_ml_node\": false,\n",
    "            \"plugins.ml_commons.connector_access_control_enabled\": true,\n",
    "            \"plugins.ml_commons.model_access_control_enabled\": true,\n",
    "            \"plugins.ml_commons.trusted_connector_endpoints_regex\": [\n",
    "              \"^https://runtime\\\\.sagemaker\\\\..*[a-z0-9-]\\\\.amazonaws\\\\.com/.*$\",\n",
    "              \"^https://api\\\\.openai\\\\.com/.*$\",\n",
    "              \"^https://api\\\\.cohere\\\\.ai/.*$\",\n",
    "              \"^https://bedrock-runtime\\\\..*[a-z0-9-]\\\\.amazonaws\\\\.com/.*$\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "    <br>\n",
    "- 部署Sparse encoding models(bi-encoder模式, 即摄入和查询的是否都进行扩词)\n",
    "  - 进入Opensearch Dashboard的dev tool， 执行如下脚本，注册bi-encoder模型\n",
    "    ```http\n",
    "    POST /_plugins/_ml/models/_register\n",
    "    {\n",
    "      \"name\": \"amazon/neural-sparse/opensearch-neural-sparse-encoding-v1\",\n",
    "      \"version\": \"1.0.1\",\n",
    "      \"model_format\": \"TORCH_SCRIPT\"\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "  - 部署模型\n",
    "    ```http\n",
    "    POST /_plugins/_ml/models/<model_id>/_deploy\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10ac26-0e20-4bfc-8987-bd0eb278da38",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 部署Sparse encoding models(doc-only模式)\n",
    "  - 进入Opensearch Dashboard的dev tool， 执行如下脚本，注册doc-only & tokenizer模型\n",
    "    + 注入侧\n",
    "        ```http\n",
    "        POST /_plugins/_ml/models/_register\n",
    "        {\n",
    "          \"name\": \"amazon/neural-sparse/opensearch-neural-sparse-encoding-doc-v1\",\n",
    "          \"version\": \"1.0.1\",\n",
    "          \"model_format\": \"TORCH_SCRIPT\"\n",
    "        }\n",
    "        ```\n",
    "    + 查询侧\n",
    "        ```http\n",
    "        POST /_plugins/_ml/models/_register\n",
    "        {\n",
    "          \"name\": \"amazon/neural-sparse/opensearch-neural-sparse-tokenizer-v1\",\n",
    "          \"version\": \"1.0.1\",\n",
    "          \"model_format\": \"TORCH_SCRIPT\"\n",
    "        }\n",
    "        ```\n",
    "  - 部署这两个模型\n",
    "    ```http\n",
    "    POST /_plugins/_ml/models/<model_id_ingest>/_deploy\n",
    "    POST /_plugins/_ml/models/<model_id_search>/_deploy\n",
    "    ```\n",
    "    + 注意：\n",
    "      + model_id_ingest 为摄入时的模型opensearch-neural-sparse-encoding-doc-v1\n",
    "      + model_id_search 为查询时的模型opensearch-neural-sparse-tokenizer-v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733d2b8-6bb7-49b0-85df-4cab5a5f795f",
   "metadata": {},
   "source": [
    "## 2. 注册Bedrock Cohere向量模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992f0d9-c5f0-42e3-8d69-33f1061021a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 检测当前账号 Bedrock Cohere模型是否可用\n",
    "  + 可以直接执行下面cell进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c2902-a1a1-4968-95ed-6a0acc3453f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BEDROCK_EMBEDDING_MODELID = \"cohere.embed-multilingual-v3\"\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)\n",
    "\n",
    "def get_embedding_bedrock(text_arrs):\n",
    "    body = json.dumps({\n",
    "        \"texts\": text_arrs,\n",
    "        \"input_type\": \"search_document\"\n",
    "    })\n",
    "    bedrock_resp = bedrock.invoke_model(\n",
    "            body=body,\n",
    "            modelId=BEDROCK_EMBEDDING_MODELID,\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "    response_body = json.loads(bedrock_resp.get('body').read())\n",
    "    embeddings = response_body['embeddings']\n",
    "    return embeddings\n",
    "\n",
    "print(get_embedding_bedrock([\"hello world\", \"see you later\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a3e1f-a679-4d50-b705-389f69bde3cb",
   "metadata": {},
   "source": [
    "- 创建connector\n",
    "  + 注意事项：\n",
    "    1. 不能直接在dev tool中执行\n",
    "    2. 不能在notebook中执行，因为AOS Domain在VPC中，需要拷贝下面代码到aos domain所在的vpc中ec2去执行\n",
    "    3. 需要给connector创建一个IAM Role : OpenSearchAndBedrockRole 参考https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ml-amazon-connector.html\n",
    "      ```json\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Sid\": \"VisualEditor0\",\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"bedrock:InvokeModel\",\n",
    "                        \"bedrock:InvokeModelWithResponseStream\",\n",
    "                        \"sagemaker:InvokeEndpointAsync\",\n",
    "                        \"sagemaker:InvokeEndpoint\"\n",
    "                    ],\n",
    "                    \"Resource\": \"*\"\n",
    "                },\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": \"iam:PassRole\",\n",
    "                    \"Resource\": \"arn:aws:iam::106839800180:role/OpenSearchAndBedrockRole\"\n",
    "                },\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": \"es:ESHttpPost\",\n",
    "                    \"Resource\": \"arn:aws:es:us-west-2:106839800180:domain/domain66ac69e0-ijsmtgwnje5s/*\"\n",
    "                }\n",
    "            ]\n",
    "        }      \n",
    "      ```\n",
    "      <br>\n",
    "  + 执行代码, 代码中有一些hardcode的变量需要进行替换,比如host, role_name, account_id, model_name。如果是用作search的input_type，需要设定为search_query\n",
    "    ```python\n",
    "    import boto3\n",
    "    import requests \n",
    "    from requests_aws4auth import AWS4Auth\n",
    "\n",
    "    service = 'es'\n",
    "    session = boto3.Session()\n",
    "    credentials = session.get_credentials()\n",
    "    region = session.region_name\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "    path = '/_plugins/_ml/connectors/_create'\n",
    "    host = 'https://vpc-domain66ac69e0-ijsmtgwnje5s-oa63og27tmacx2fstb5hpstta4.us-west-2.es.amazonaws.com'\n",
    "    url = host + path\n",
    "\n",
    "    role_name = \"OpenSearchAndBedrockRole\"\n",
    "    account_id = \"106839800180\"\n",
    "    role_arn = \"arn:aws:iam::{}:role/{}\".format(account_id, role_name)\n",
    "    model_name = \"cohere.embed-multilingual-v3\"\n",
    "\n",
    "    bedrock_url = \"https://bedrock-runtime.{}.amazonaws.com/model/{}/invoke\".format(region, model_name)\n",
    "\n",
    "    payload = {\n",
    "      \"name\": \"Amazon Bedrock Connector: Cohere doc embedding\",\n",
    "      \"description\": \"The connector to the Bedrock Cohere multilingual doc embedding model\",\n",
    "      \"version\": 1,\n",
    "      \"protocol\": \"aws_sigv4\",\n",
    "      \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\"\n",
    "      },\n",
    "      \"credential\": {\n",
    "        \"roleArn\": role_arn\n",
    "      },\n",
    "      \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": bedrock_url,\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "          \"request_body\": \"{ \\\"texts\\\": ${parameters.texts}, \\\"input_type\\\": \\\"search_document\\\" }\",\n",
    "          \"pre_process_function\": \"connector.pre_process.cohere.embedding\",\n",
    "          \"post_process_function\": \"connector.post_process.cohere.embedding\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "    ```\n",
    "    <br>\n",
    "  + 输出 <br>\n",
    "    + 用于向量化文档的connector\n",
    "        ```json\n",
    "        {\"connector_id\":\"3ktHC4wB0S9ucTLoRFvx\"}\n",
    "        ```\n",
    "        <br>\n",
    "    + 用于向量化query的connector <br> \n",
    "        ```json\n",
    "        {\"connector_id\":\"4kt2C4wB0S9ucTLoNVsP\"}\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4d54c-1ffa-4c0a-b38e-3355ab531d88",
   "metadata": {},
   "source": [
    "- 注册外部模型 <br>\n",
    "    注意替换对应的变量 <br>\n",
    "    可以添加deploy=true参数，也可以通过`POST /_plugins/_ml/models/<model_id>/_deploy` 进行部署\n",
    "    ```http\n",
    "    POST /_plugins/_ml/models/_register?deploy=true\n",
    "    {\n",
    "        \"name\": \"cohere embed-multilingual-v3\",\n",
    "        \"function_name\": \"remote\",\n",
    "        \"model_group_id\": \"zksL94sB0S9ucTLoj1u0\",\n",
    "        \"description\": \"embedding for multilingual\",\n",
    "        \"connector_id\": \"3ktHC4wB0S9ucTLoRFvx\"\n",
    "    }\n",
    "    ```\n",
    "    输出: \n",
    "    ```json\n",
    "    {\n",
    "      \"task_id\": \"30tIC4wB0S9ucTLoWVtV\",\n",
    "      \"status\": \"CREATED\",\n",
    "      \"model_id\": \"4EtIC4wB0S9ucTLoWVtt\" // for doc: 4EtIC4wB0S9ucTLoWVtt; for query: 5Et6C4wB0S9ucTLo1Vsj\n",
    "    }\n",
    "    ```\n",
    "\n",
    "- 测试外部模型\n",
    "    ```http\n",
    "    POST /_plugins/_ml/models/<model_id>/_predict\n",
    "    {\n",
    "      \"parameters\": {\n",
    "        \"texts\": [\"Hello word\", \"Hi Altman\"]\n",
    "      }\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04701d95-46b0-4163-816a-df1afe47add9",
   "metadata": {},
   "source": [
    "## 3. 在摄入阶段配置Sparse encoding models\n",
    "\n",
    "- 构建摄入的pipeline\n",
    "```http\n",
    "PUT /_ingest/pipeline/neural-sparse-pipeline\n",
    "{\n",
    "  \"description\": \"neural sparse encoding pipeline\",\n",
    "  \"processors\" : [\n",
    "    {\n",
    "      \"sparse_encoding\": {\n",
    "        \"model_id\": \"<nerual_sparse_model_id>\",\n",
    "        \"field_map\": {\n",
    "           \"content\": \"sparse_embedding\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": \"<cohere_ingest_model_id>\",\n",
    "        \"field_map\": {\n",
    "          \"doc\": \"embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de473090-a6e0-46e7-9f63-5d8c772f6310",
   "metadata": {},
   "source": [
    "- 构建包含Sparse encoding的OpenSearch Index\n",
    "```http\n",
    "PUT chatbot-index\n",
    "{\n",
    "    \"settings\" : {\n",
    "        \"index\":{\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 0,\n",
    "            \"knn\": \"true\",\n",
    "            \"knn.algo_param.ef_search\": 32\n",
    "        }, \n",
    "        \"default_pipeline\": \"neural-sparse-pipeline\"\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"publish_date\" : {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd HH:mm:ss\"\n",
    "            },\n",
    "            \"idx\" : {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"doc_type\" : {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"doc\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ik_max_word\",\n",
    "                \"search_analyzer\": \"ik_smart\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ik_max_word\",\n",
    "                \"search_analyzer\": \"ik_smart\"\n",
    "            },\n",
    "            \"doc_title\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"doc_author\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"doc_category\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"engine\": \"nmslib\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"parameters\": {}\n",
    "                }            \n",
    "            },\n",
    "            \"sparse_embedding\": {\n",
    "                \"type\": \"rank_features\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eaeff-d8a5-4686-aea4-4caa10ef65b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. 摄入测试\n",
    "\n",
    "```http\n",
    "PUT /chatbot-index/_doc/1\n",
    "{\n",
    "    \"publish_date\" : \"2023-11-22 10:00:00\",\n",
    "    \"idx\" : 1,\n",
    "    \"doc_type\" : \"paragraph\",\n",
    "    \"doc\" : \"Sam Altman returns to OpenAI in a bizarre reversal of fortunes\",\n",
    "    \"content\" : \"New York - Sam Altman has agreed to return to lead OpenAI, the company said in a Tuesday post on X, just days after his surprise ouster as chief executive sparked an employee revolt that threatened to undermine what has been the leading company in the fledgling artificial intelligence industry.\",\n",
    "    \"doc_title\" : \"\",\n",
    "    \"doc_author\" : \"Altman\",\n",
    "    \"doc_category\" : \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5. 查询测试\n",
    "- 测试 neural_sparse 查询\n",
    "```http\n",
    "GET /chatbot-index/_search?explain=true\n",
    "{\n",
    "  \"query\": {\n",
    "      \"neural_sparse\": {\n",
    "          \"sparse_embedding\": {\n",
    "            \"query_text\": \"OpenAI Inc\",\n",
    "            \"model_id\": \"<nerual_sparse_model_id>\",\n",
    "            \"max_token_score\": 3.5\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- 测试Cohere neural_dense 查询 \n",
    "```http\n",
    "GET /chatbot-index/_search/\n",
    "{\n",
    "    \"query\": {\n",
    "        \"neural\": {\n",
    "            \"embedding\": {\n",
    "              \"query_text\": \"OpenAI Inc\",\n",
    "              \"model_id\": \"<cohere_search_model_id>\",\n",
    "              \"k\": 10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "- 测试两路混合查询\n",
    "  - 创建search_pipeline\n",
    "    ```http\n",
    "    PUT /_search/pipeline/dense-sparse-pipeline\n",
    "    {\n",
    "      \"description\": \"Post processor for hybrid search\",\n",
    "      \"phase_results_processors\": [\n",
    "        {\n",
    "          \"normalization-processor\": {\n",
    "            \"normalization\": {\n",
    "              \"technique\": \"l2\"\n",
    "            },\n",
    "            \"combination\": {\n",
    "              \"technique\": \"arithmetic_mean\",\n",
    "              \"parameters\": {\n",
    "                \"weights\": [\n",
    "                  0.3,\n",
    "                  0.7\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "    <br>\n",
    "  - 执行混合查询 <br>\n",
    "    ```http\n",
    "    GET /chatbot-index/_search?search_pipeline=dense-sparse-pipeline\n",
    "    {\n",
    "      \"query\": {\n",
    "        \"hybrid\": {\n",
    "          \"queries\": [\n",
    "            {\n",
    "              \"neural_sparse\": {\n",
    "                  \"sparse_embedding\": {\n",
    "                    \"query_text\": \"OpenAI Inc\",\n",
    "                    \"model_id\": \"<splade_model_id>\",\n",
    "                    \"max_token_score\": 2.0\n",
    "                  }\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"neural\": {\n",
    "                \"embedding\": {\n",
    "                  \"query_text\": \"OpenAI Inc\",\n",
    "                  \"model_id\": \"<cohere_search_model_id>\",\n",
    "                  \"k\": 10\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92a11b-70bc-4187-8c34-ed4305b3958f",
   "metadata": {},
   "source": [
    "## 6. 如何删除模型与connector\n",
    "\n",
    "- 卸载部署的外部模型(如果需要重续部署，可以先卸载)\n",
    "    ```http\n",
    "    POST /_plugins/_ml/models/<model_id>/_undeploy\n",
    "    ```\n",
    "    输出 =>\n",
    "    ```json\n",
    "    {\n",
    "      \"W5BqyJqbRr2GPVkjwgoaqQ\": {\n",
    "        \"stats\": {\n",
    "          \"vdkQ94sBMoMEFe1F4cRP\": \"undeployed\"\n",
    "        }\n",
    "      },\n",
    "      \"dbjOCw5sSBuIKgKz3CKXjQ\": {\n",
    "        \"stats\": {\n",
    "          \"vdkQ94sBMoMEFe1F4cRP\": \"undeployed\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    ```\n",
    "    <br>\n",
    "- 删除这个模型(_register的反向操作)\n",
    "    ```http\n",
    "    DELETE /_plugins/_ml/models/<model_id> # v9kB-4sBMoMEFe1FCcRr\n",
    "    ```\n",
    "    输出 => \n",
    "    ```json\n",
    "    {\n",
    "      \"_index\": \".plugins-ml-model\",\n",
    "      \"_id\": \"v9kB-4sBMoMEFe1FCcRr\",\n",
    "      \"_version\": 5,\n",
    "      \"result\": \"deleted\",\n",
    "      \"_shards\": {\n",
    "        \"total\": 2,\n",
    "        \"successful\": 2,\n",
    "        \"failed\": 0\n",
    "      },\n",
    "      \"_seq_no\": 4,\n",
    "      \"_primary_term\": 1\n",
    "    }\n",
    "    ```\n",
    "    <br>\n",
    "- 删除connector (重新创建的时候)\n",
    "    ```http\n",
    "    DELETE /_plugins/_ml/connectors/<connector_id>\n",
    "    ```\n",
    "    <br>输出=>\n",
    "    ```json\n",
    "    {\n",
    "      \"_index\": \".plugins-ml-connector\",\n",
    "      \"_id\": \"u9lo9osBMoMEFe1FhcTh\",\n",
    "      \"_version\": 2,\n",
    "      \"result\": \"deleted\",\n",
    "      \"_shards\": {\n",
    "        \"total\": 2,\n",
    "        \"successful\": 2,\n",
    "        \"failed\": 0\n",
    "      },\n",
    "      \"_seq_no\": 1,\n",
    "      \"_primary_term\": 1\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752352e-7f54-4804-b4a6-1f2d53c17bfd",
   "metadata": {},
   "source": [
    "## 7. FAQ\n",
    "\n",
    "问：关于批量推理? <br>\n",
    "答：似乎nerual-sparse模型不能批量推理， bedrock: Cohere Embedding可以批量推理\n",
    "\n",
    "问：Cohere模型的传入参数有哪些讲究？<br>\n",
    "答：不能完全参考cohere官网，可以传入texts和query_type, truncate不能传入，否则会报错。modelId无需传入\n",
    "\n",
    "问：使用nerual-sparse模型的版本要求 <br>\n",
    "答：2.11版本以上，2.11版本通过sagemaker接入（ml.g4dn.xlarge具有最佳性价比），2.12版本预计可以部署在OpenSearch集群中\n",
    "\n",
    "问: 需要部署哪些模型？<br>\n",
    "答：如果Nerual-Sparse采用bi-encoder(也就是查询和文档都进行term expansion),那么仅需要部署一个模型。 如果采用doc-only，那么需要部署两个模型，一个用于对文档扩词，一个用于对query进行分词。对于Cohere, 由于调用的是bedrock API，所以本质上没有模型部署，但由于对doc和query向量化传入的参数不同，所以需要创建两个connector，并构建两个OpenSearch的model\n",
    "\n",
    "问: 构建connector时，pre_process_function 和 post_process_function分别起什么作用？<br>\n",
    "答：pre_process_function用于把OpenSearch Pipeline中的一些输入格式转换成模型接口的参数形式。post_process_function用于把模型的response中间结果转换成pipeline所需要的格式。建议使用OpenSearch中的一些定义好的Processor，模型接口变化后，OpenSearch也会随之进行维护。\n",
    "\n",
    "问: nerual sparse 查询中的 max_token_score 参数是做什么用的？<br>\n",
    "答: 这个代表了倒排索引上一个token能贡献的分数上限，用来适配lucene的WAND剪枝算法的，可以加速query。对我们的2个模型就用固定的2和3.5就行，不影响检索结果，纯加速用的。3.5 是给bi-encoder模式的固定参数，2是给doc-only模式的固定参数。 下个aos版本lucene做了不少优化，我们也不再需要，就会deprecate这个参数\n",
    "\n",
    "问: neural_sparse 能使用explain 吗？\n",
    "答: 可以，但是hybird查询的时不能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854ec1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
